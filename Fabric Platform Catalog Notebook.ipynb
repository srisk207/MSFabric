{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MICROSOFT FABRIC PLATFORM INVENTORY & CATALOG\n",
    "### Description:\n",
    "###   This script scans a Microsoft Fabric tenant to inventory all items (Workspaces, \n",
    "###   Lakehouses, Warehouses, Reports, etc.), maps lineage, audits security (Users/Roles), \n",
    "###   and parses deep metadata (TMSL) for Semantic Models.\n",
    "### Prerequisites:\n",
    "###   - Microsoft Fabric Capacity\n",
    "###   - 'sempy' library installed (pip install semantic-link)\n",
    "###   - Spark Session active (Run in Fabric Notebook)\n",
    "### Warning Freeze cell [Get folders in workspace] takes longtime to run\n",
    "### =============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Fabric libraries and set Spark configuration\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean up column headers\n",
    "# Extracts text inside brackets [ ] if present, otherwise returns original header\n",
    "def extract_text_between_brackets(header):\n",
    "    import re\n",
    "    match = re.search(r'\\[(.*?)\\]', header)\n",
    "    return match.group(1) if match else header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabric All Items load a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "#!pip install --upgrade semantic-link --q #upgrade to semantic-link v0.5\n",
    "\n",
    "df = pd.concat([fabric.list_items(workspace=ws) for ws in fabric.list_workspaces().query('`Is On Dedicated Capacity` == True').Id], ignore_index=True)\n",
    "df\n",
    "def clean_data(df):\n",
    "    # Rename column 'Display Name' to 'DisplayName'\n",
    "    df = df.rename(columns={'Display Name': 'DisplayName'})\n",
    "    # Rename column 'Workspace Id' to 'WorkspaceId'\n",
    "    df = df.rename(columns={'Workspace Id': 'WorkspaceId'})\n",
    "    df = df.rename(columns={'Folder Id': 'FolderId'})\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df2 = spark.createDataFrame(df_clean)\n",
    "df2.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"FABRIC_ALLITEMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all Semantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "\n",
    "# Get all workspaces\n",
    "workspaces_df = fabric.list_workspaces()\n",
    "\n",
    "# Filter workspaces on dedicated capacity\n",
    "dedicated_ws_ids = workspaces_df.query(\"`Is On Dedicated Capacity` == True\")[\"Id\"]\n",
    "\n",
    "# Collect datasets and include workspace ID\n",
    "dataset_dfs = []\n",
    "for ws_id in dedicated_ws_ids:\n",
    "    df = fabric.list_datasets(workspace=ws_id, mode='rest')\n",
    "    df[\"WorkspaceId\"] = ws_id  # Add workspace ID to each dataset row\n",
    "    dataset_dfs.append(df)\n",
    "\n",
    "# Combine all datasets\n",
    "df_dax = pd.concat(dataset_dfs, ignore_index=True)\n",
    "df_dax = df_dax.astype(str)\n",
    "\n",
    "# Clean and prepare the DataFrame\n",
    "df_dax.columns = df_dax.columns.str.replace(\" \", \"\", regex=False)\n",
    "#df_dax = df_dax.drop(columns=['UpstreamDatasets', 'Users','QueryScaleOutSettings'])\n",
    "\n",
    "# Convert to Spark DataFrame and save\n",
    "df2 = spark.createDataFrame(df_dax)\n",
    "df2.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"LIST_DATASETS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all PowerBI Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "workspaces_df = fabric.list_workspaces()\n",
    "dedicated_ws_ids = workspaces_df.query(\"`Is On Dedicated Capacity` == True\")[\"Id\"].tolist()\n",
    "\n",
    "def get_reports_safe(ws_id):\n",
    "    try:\n",
    "        df = fabric.list_reports(workspace=ws_id)\n",
    "        if not df.empty:\n",
    "            df[\"WorkspaceId\"] = ws_id\n",
    "            return df\n",
    "    except:\n",
    "        return None # Return None on error\n",
    "    return None # Return None if empty\n",
    "\n",
    "# Run in parallel\n",
    "report_dfs = []\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = executor.map(get_reports_safe, dedicated_ws_ids)\n",
    "    # Filter out None values (errors or empty workspaces)\n",
    "    report_dfs = [r for r in results if r is not None]\n",
    "\n",
    "if report_dfs:\n",
    "    df_reports = pd.concat(report_dfs, ignore_index=True)\n",
    "    df_reports = df_reports.astype(str)\n",
    "    df_reports.columns = df_reports.columns.str.replace(\" \", \"\", regex=False)\n",
    "    \n",
    "    spark.createDataFrame(df_reports).write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"LIST_REPORTS\")\n",
    "    print(\"Table LIST_REPORTS updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all Dataflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "\n",
    "# Get all workspaces\n",
    "workspaces_df = fabric.list_workspaces()\n",
    "\n",
    "# Filter workspaces on dedicated capacity\n",
    "dedicated_ws_ids = workspaces_df.query(\"`Is On Dedicated Capacity` == True\")[\"Id\"]\n",
    "\n",
    "# Collect datasets and include workspace ID\n",
    "dataset_dfs = []\n",
    "for ws_id in dedicated_ws_ids:\n",
    "    df = fabric.list_dataflows(workspace=ws_id)\n",
    "    df[\"WorkspaceId\"] = ws_id  # Add workspace ID to each dataset row\n",
    "    dataset_dfs.append(df)\n",
    "\n",
    "# Combine all datasets\n",
    "df_dax = pd.concat(dataset_dfs, ignore_index=True)\n",
    "\n",
    "# Clean and prepare the DataFrame\n",
    "df_dax.columns = df_dax.columns.str.replace(\" \", \"\", regex=False)\n",
    "#df_dax = df_dax.drop(columns=['UpstreamDatasets', 'Users','QueryScaleOutSettings'])\n",
    "\n",
    "# Convert to Spark DataFrame and save\n",
    "df2 = spark.createDataFrame(df_dax)\n",
    "df2.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"LIST_DATAFLOWS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get folders in workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "\n",
    "# Get all workspaces\n",
    "workspaces_df = fabric.list_workspaces()\n",
    "\n",
    "# Filter workspaces that are on dedicated capacity\n",
    "dedicated_ws_ids = workspaces_df.query(\"`Is On Dedicated Capacity` == True\")[\"Id\"]\n",
    "\n",
    "# Collect folders from each workspace\n",
    "folder_dfs = []\n",
    "for ws_id in dedicated_ws_ids:\n",
    "    folders = fabric.list_folders(workspace=ws_id)\n",
    "    folder_dfs.append(folders)\n",
    "\n",
    "# Concatenate all folder dataframes\n",
    "df = pd.concat(folder_dfs, ignore_index=True)\n",
    "\n",
    "# Display the result\n",
    "print(df)\n",
    "\n",
    "def clean_data(df):\n",
    "    # Rename column 'Display Name' to 'DisplayName'\n",
    "    df = df.rename(columns={'Display Name': 'DisplayName'})\n",
    "    # Rename column 'Workspace Id' to 'WorkspaceId'\n",
    "    df = df.rename(columns={'Workspace Id': 'WorkspaceId'})\n",
    "    df = df.rename(columns={'Parent Folder Id': 'ParentFolderId'})\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df2 = spark.createDataFrame(df_clean)\n",
    "df2.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"FABRIC_WSPFOLDERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Workspace Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "#workspaceName = '' #Enter the workspace name to be used as a filter\n",
    "df_dax = fabric.list_workspaces()\n",
    "\n",
    "df_dax.columns = [extract_text_between_brackets(col) for col in df_dax.columns]\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Add a new column to df_dax with the current date and time\n",
    "df_dax['Update_DateTime'] = current_datetime\n",
    "\n",
    "# Display the updated dataframe with the new column\n",
    "df_dax.columns =df_dax.columns.str.replace(\"(\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\")\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\" \", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\"\\n\", \"\", regex=False)\n",
    "df_dax\n",
    "print(df_dax)\n",
    "df_dax = spark.createDataFrame(df_dax)\n",
    "df_dax.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"WORKSPACE_DETAILS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dax=fabric.list_capacities()\n",
    "# Display the updated dataframe with the new column\n",
    "df_dax.columns =df_dax.columns.str.replace(\"(\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\")\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\" \", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\"\\n\", \"\", regex=False)\n",
    "df_dax\n",
    "#print(df_dax)\n",
    "df_dax = spark.createDataFrame(df_dax)\n",
    "df_dax.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"CAPACITY_DETAILS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "df_dax = fabric.list_apps()\n",
    "# Display the updated dataframe with the new column\n",
    "df_dax.columns =df_dax.columns.str.replace(\"(\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\")\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\" \", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\"\\n\", \"\", regex=False)\n",
    "df_dax\n",
    "#print(df_dax)\n",
    "df_dax = spark.createDataFrame(df_dax)\n",
    "df_dax.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"APP_DETAILS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateway Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "df_dax = fabric.list_gateways()\n",
    "df_dax.columns =df_dax.columns.str.replace(\"(\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\")\", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\" \", \"\", regex=False)\n",
    "df_dax.columns =df_dax.columns.str.replace(\"\\n\", \"\", regex=False)\n",
    "df_dax\n",
    "#print(df_dax)\n",
    "df_dax = spark.createDataFrame(df_dax)\n",
    "df_dax.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"GATEWAY_DETAILS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakehouse Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_lakehouse_details():\n",
    "    header = pd.DataFrame(columns=['LakehouseName', 'LakehouseID', 'WorkspaceName', 'WorkspaceID', 'OneLakeTablesPath', 'OneLakeFilesPath', 'SQLEndpointConnectionString', 'SQLEndpointID', 'SQLEndpointProvisioningStatus'])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            items_response = client.get(f\"/v1/workspaces/{workspaceID}/items\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Lakehouses\n",
    "            lakehouses = [item for item in items if item['type'] == 'Lakehouse']\n",
    "            \n",
    "            # Process each Lakehouse\n",
    "            for lakehouse in lakehouses:\n",
    "                lakehouseID = lakehouse['id']\n",
    "                \n",
    "                try:\n",
    "                    # Get Lakehouse details\n",
    "                    response = client.get(f\"/v1/workspaces/{workspaceID}/lakehouses/{lakehouseID}\")\n",
    "                    responseJson = response.json()\n",
    "                    lakehouseName = responseJson['displayName']\n",
    "                    prop = responseJson['properties']\n",
    "                    oneLakeTP = prop['oneLakeTablesPath']\n",
    "                    oneLakeFP = prop['oneLakeFilesPath']\n",
    "                    sqlEPCS = prop['sqlEndpointProperties']['connectionString']\n",
    "                    sqlepid = prop['sqlEndpointProperties']['id']\n",
    "                    sqlepstatus = prop['sqlEndpointProperties']['provisioningStatus']\n",
    "                    \n",
    "                    new_data = {\n",
    "                        'LakehouseName': lakehouseName, \n",
    "                        'LakehouseID': lakehouseID, \n",
    "                        'WorkspaceName': workspaceName, \n",
    "                        'WorkspaceID': workspaceID, \n",
    "                        'OneLakeTablesPath': oneLakeTP, \n",
    "                        'OneLakeFilesPath': oneLakeFP, \n",
    "                        'SQLEndpointConnectionString': sqlEPCS, \n",
    "                        'SQLEndpointID': sqlepid, \n",
    "                        'SQLEndpointProvisioningStatus': sqlepstatus\n",
    "                    }\n",
    "                    df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Lakehouse {lakehouseID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_LH = spark.createDataFrame(df)\n",
    "    return df_LH\n",
    "\n",
    "# Call the function\n",
    "df_all_lakehouses = get_all_lakehouse_details()\n",
    "df_all_lakehouses.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"LAKEHOUSE_DETAILS\")\n",
    "#display(df_all_lakehouses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warehouse details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_warehouse_details():\n",
    "    header = pd.DataFrame(columns=['WarehouseName', 'WarehouseID', 'WorkspaceName', 'WorkspaceID', 'ConnectionString', 'CreatedDate', 'lastUpdatedTime'])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            items_response = client.get(f\"/v1/workspaces/{workspaceID}/items\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Warehouses\n",
    "            warehouses = [item for item in items if item['type'] == 'Warehouse']\n",
    "            \n",
    "            # Process each Warehouse\n",
    "            for warehouse in warehouses:\n",
    "                warehouseID = warehouse['id']\n",
    "                warehouseName = warehouse['displayName']\n",
    "                \n",
    "                try:\n",
    "                    # Get Warehouse details\n",
    "                    response = client.get(f\"/v1/workspaces/{workspaceID}/warehouses/{warehouseID}\")\n",
    "                    responseJson = response.json()\n",
    "                    \n",
    "                    warehouseName = responseJson['displayName']\n",
    "                                      \n",
    "                    # Get connection string from properties\n",
    "                    prop = responseJson.get('properties', {})\n",
    "                    connectionString = prop.get('connectionString', 'N/A')\n",
    "                    createdDate = prop.get('createdDate', 'N/A')\n",
    "                    lastUpdatedTime = prop.get('lastUpdatedTime', 'N/A')\n",
    "                    \n",
    "                    new_data = {\n",
    "                        'WarehouseName': warehouseName, \n",
    "                        'WarehouseID': warehouseID, \n",
    "                        'WorkspaceName': workspaceName, \n",
    "                        'WorkspaceID': workspaceID, \n",
    "                        'ConnectionString': connectionString,\n",
    "                        'CreatedDate': createdDate,\n",
    "                        'lastUpdatedTime': lastUpdatedTime\n",
    "                    }\n",
    "                    df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Warehouse {warehouseID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_WH = spark.createDataFrame(df)\n",
    "    return df_WH\n",
    "\n",
    "# Call the function\n",
    "df_all_warehouses = get_all_warehouse_details()\n",
    "df_all_warehouses.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"WAREHOUSE_DETAILS\")\n",
    "# display(df_all_warehouses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset - Data source Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_dataset_details():\n",
    "    header = pd.DataFrame(columns=[\n",
    "        'DatasetName', 'DatasetID', 'WorkspaceName', 'WorkspaceID', \n",
    "        'isRefreshable', 'ConfiguredBy', 'isOnPremGatewayRequired', \n",
    "        'targetStorageMode', 'DatasourceId', 'DatasourceType', \n",
    "        'GatewayId', 'ConnectionDetails'\n",
    "    ])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    pbiclient = fabric.PowerBIRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Filter workspaces on dedicated capacity\n",
    "    workspaces = [\n",
    "        ws for ws in workspaces \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000'\n",
    "    ]\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            items_response = client.get(f\"/v1/workspaces/{workspaceID}/items\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Datasets (SemanticModel)\n",
    "            datasets = [item for item in items if item['type'] == 'SemanticModel']\n",
    "            \n",
    "            # Process each Dataset\n",
    "            for dataset in datasets:\n",
    "                datasetID = dataset['id']\n",
    "                datasetName = dataset['displayName']\n",
    "                \n",
    "                try:\n",
    "                    # Get Dataset details\n",
    "                    response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/datasets/{datasetID}\")\n",
    "                    responseJson = response.json()\n",
    "                    \n",
    "                    datasetName = responseJson['name']\n",
    "                    isRefreshable = responseJson.get('isRefreshable', 'N/A')\n",
    "                    isOnPremGatewayRequired = responseJson.get('isOnPremGatewayRequired', 'N/A')\n",
    "                    configuredBy = responseJson.get('configuredBy', 'N/A')\n",
    "                    targetStorageMode = responseJson.get('targetStorageMode', 'N/A')\n",
    "                    \n",
    "                    # Get datasources/connections\n",
    "                    try:\n",
    "                        datasources_response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/datasets/{datasetID}/datasources\")\n",
    "                        datasources = datasources_response.json().get('value', [])\n",
    "                        \n",
    "                        # If datasources exist, create one row per datasource\n",
    "                        if datasources:\n",
    "                            for ds in datasources:\n",
    "                                datasource_type = ds.get('datasourceType', 'N/A')\n",
    "                                datasource_id = ds.get('datasourceId', 'N/A')\n",
    "                                gateway_id = ds.get('gatewayId', 'N/A')\n",
    "                                connection_details = json.dumps(ds.get('connectionDetails', {}))\n",
    "                                \n",
    "                                new_data = {\n",
    "                                    'DatasetName': datasetName, \n",
    "                                    'DatasetID': datasetID, \n",
    "                                    'WorkspaceName': workspaceName, \n",
    "                                    'WorkspaceID': workspaceID,\n",
    "                                    'ConfiguredBy': configuredBy,\n",
    "                                    'targetStorageMode': targetStorageMode,\n",
    "                                    'isRefreshable': isRefreshable,\n",
    "                                    'isOnPremGatewayRequired': isOnPremGatewayRequired,\n",
    "                                    'DatasourceId': datasource_id,\n",
    "                                    'DatasourceType': datasource_type,\n",
    "                                    'GatewayId': gateway_id,\n",
    "                                    'ConnectionDetails': connection_details\n",
    "                                }\n",
    "                                df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                        else:\n",
    "                            # No datasources found - create one row with N/A values\n",
    "                            new_data = {\n",
    "                                'DatasetName': datasetName, \n",
    "                                'DatasetID': datasetID, \n",
    "                                'WorkspaceName': workspaceName, \n",
    "                                'WorkspaceID': workspaceID,\n",
    "                                'ConfiguredBy': configuredBy,\n",
    "                                'targetStorageMode': targetStorageMode,\n",
    "                                'isRefreshable': isRefreshable,\n",
    "                                'isOnPremGatewayRequired': isOnPremGatewayRequired,\n",
    "                                'DatasourceId': 'N/A',\n",
    "                                'DatasourceType': 'N/A',\n",
    "                                'GatewayId': 'N/A',\n",
    "                                'ConnectionDetails': 'N/A'\n",
    "                            }\n",
    "                            df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                            \n",
    "                    except Exception as conn_error:\n",
    "                        # Error getting datasources - create one row with error\n",
    "                        new_data = {\n",
    "                            'DatasetName': datasetName, \n",
    "                            'DatasetID': datasetID, \n",
    "                            'WorkspaceName': workspaceName, \n",
    "                            'WorkspaceID': workspaceID,\n",
    "                            'ConfiguredBy': configuredBy,\n",
    "                            'targetStorageMode': targetStorageMode,\n",
    "                            'isRefreshable': isRefreshable,\n",
    "                            'isOnPremGatewayRequired': isOnPremGatewayRequired,\n",
    "                            'DatasourceId': 'Error',\n",
    "                            'DatasourceType': 'Error',\n",
    "                            'GatewayId': 'Error',\n",
    "                            'ConnectionDetails': str(conn_error)\n",
    "                        }\n",
    "                        df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Dataset {datasetID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_DS = spark.createDataFrame(df)\n",
    "    return df_DS\n",
    "\n",
    "# Call the function\n",
    "df_all_datasets = get_all_dataset_details()\n",
    "df_all_datasets.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DATASET_DATASOURCES\")\n",
    "#display(df_all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow - Data source Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_dataset_details():\n",
    "    header = pd.DataFrame(columns=[\n",
    "        'DataflowName', 'DataflowID', 'WorkspaceName', 'WorkspaceID', \n",
    "        'ConfiguredBy', 'generation', \n",
    "         'DatasourceId', 'DatasourceType', \n",
    "        'GatewayId', 'ConnectionDetails'\n",
    "    ])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    pbiclient = fabric.PowerBIRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    # workspaces_response = pbiclient.get(\"/v1.0/myorg/groups\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Filter workspaces on dedicated capacity\n",
    "    workspaces = [\n",
    "        ws for ws in workspaces \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000'\n",
    "    ]\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            # items_response = client.get(f\"/v1/workspaces/{workspaceID}/items\")\n",
    "            items_response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/dataflows\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Datasets (SemanticModel)\n",
    "            dataflows = items\n",
    "            # dataflows = [item for item in items if item['type'] == 'Dataflow']\n",
    "            \n",
    "            # Process each Dataset\n",
    "            for dataflow in dataflows:\n",
    "                dataflowID = dataflow['objectId']\n",
    "                dataflowName = dataflow['name']\n",
    "                configuredBy = dataflow['configuredBy']\n",
    "                generation = dataflow['generation']\n",
    "                try:\n",
    "                    # Get Dataset details\n",
    "                    response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/dataflows/{dataflowID}\")\n",
    "                    responseJson = response.json()\n",
    "                    \n",
    "                    dataflowName = responseJson['name']\n",
    "                    #isRefreshable = responseJson.get('isRefreshable', 'N/A')\n",
    "                    #isOnPremGatewayRequired = responseJson.get('isOnPremGatewayRequired', 'N/A')\n",
    "                    #configuredBy = responseJson.get('configuredBy', 'N/A')\n",
    "                    #targetStorageMode = responseJson.get('targetStorageMode', 'N/A')\n",
    "                    \n",
    "                    # Get datasources/connections\n",
    "                    try:\n",
    "                        datasources_response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/dataflows/{dataflowID}/datasources\")\n",
    "                        datasources = datasources_response.json().get('value', [])\n",
    "                        \n",
    "                        # If datasources exist, create one row per datasource\n",
    "                        if datasources:\n",
    "                            for ds in datasources:\n",
    "                                datasource_type = ds.get('datasourceType', 'N/A')\n",
    "                                datasource_id = ds.get('datasourceId', 'N/A')\n",
    "                                gateway_id = ds.get('gatewayId', 'N/A')\n",
    "                                connection_details = json.dumps(ds.get('connectionDetails', {}))\n",
    "                                \n",
    "                                new_data = {\n",
    "                                    'DataflowName': dataflowName, \n",
    "                                    'DataflowID': dataflowID, \n",
    "                                    'WorkspaceName': workspaceName, \n",
    "                                    'WorkspaceID': workspaceID,\n",
    "                                    'ConfiguredBy': configuredBy,\n",
    "                                    'generation': generation,\n",
    "                                    'DatasourceId': datasource_id,\n",
    "                                    'DatasourceType': datasource_type,\n",
    "                                    'GatewayId': gateway_id,\n",
    "                                    'ConnectionDetails': connection_details\n",
    "                                }\n",
    "                                df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                        else:\n",
    "                            # No datasources found - create one row with N/A values\n",
    "                            new_data = {\n",
    "                                'DataflowName': dataflowName, \n",
    "                                'DataflowID': dataflowID, \n",
    "                                'WorkspaceName': workspaceName, \n",
    "                                'WorkspaceID': workspaceID,\n",
    "                                'ConfiguredBy': configuredBy,\n",
    "                                'generation': generation,\n",
    "                                'DatasourceId': 'N/A',\n",
    "                                'DatasourceType': 'N/A',\n",
    "                                'GatewayId': 'N/A',\n",
    "                                'ConnectionDetails': 'N/A'\n",
    "                            }\n",
    "                            df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                            \n",
    "                    except Exception as conn_error:\n",
    "                        # Error getting datasources - create one row with error\n",
    "                        new_data = {\n",
    "                            'DataflowName': dataflowName, \n",
    "                            'DataflowID': dataflowID, \n",
    "                            'WorkspaceName': workspaceName, \n",
    "                            'WorkspaceID': workspaceID,\n",
    "                            'ConfiguredBy': configuredBy,\n",
    "                            'generation': generation,\n",
    "                            'DatasourceId': 'Error',\n",
    "                            'DatasourceType': 'Error',\n",
    "                            'GatewayId': 'Error',\n",
    "                            'ConnectionDetails': str(conn_error)\n",
    "                        }\n",
    "                        df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Dataset {dataflowID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_DS = spark.createDataFrame(df)\n",
    "    return df_DS\n",
    "\n",
    "# Call the function\n",
    "df_all_datasets = get_all_dataset_details()\n",
    "df_all_datasets.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DATAFLOW_DATASOURCES\")\n",
    "#display(df_all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All DataFlows Gen1, 2 CICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_dataflow_details():\n",
    "    header = pd.DataFrame(columns=[\n",
    "        'DataflowName', 'DataflowID', 'WorkspaceName', 'WorkspaceID', \n",
    "        'ConfiguredBy', 'generation', \n",
    "         'Type', 'Call','isParametric'\n",
    "    ])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    pbiclient = fabric.PowerBIRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    # workspaces_response = pbiclient.get(\"/v1.0/myorg/groups\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Filter workspaces on dedicated capacity\n",
    "    workspaces = [\n",
    "        ws for ws in workspaces \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000'\n",
    "    ]\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            # items_response = client.get(f\"/v1/workspaces/{workspaceID}/items\")\n",
    "            items_response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/dataflows\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Datasets (SemanticModel)\n",
    "            dataflows = items\n",
    "            # dataflows = [item for item in items if item['type'] == 'Dataflow']\n",
    "            \n",
    "            # Process each Dataset\n",
    "            for dataflow in dataflows:\n",
    "                dataflowID = dataflow['objectId']\n",
    "                dataflowName = dataflow['name']\n",
    "                configuredBy = dataflow['configuredBy']\n",
    "                generation = dataflow['generation']\n",
    "                try:\n",
    "                    # Error getting datasources - create one row with error\n",
    "                        new_data = {\n",
    "                            'DataflowName': dataflowName, \n",
    "                            'DataflowID': dataflowID, \n",
    "                            'WorkspaceName': workspaceName, \n",
    "                            'WorkspaceID': workspaceID,\n",
    "                            'ConfiguredBy': configuredBy,\n",
    "                            'generation': generation,\n",
    "                            'Type': 'Dataflow',\n",
    "                            'Call':'PBIRESTAPI'\n",
    "                            }\n",
    "                        df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Dataset {dataflowID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_DS = spark.createDataFrame(df)\n",
    "    return df_DS\n",
    "\n",
    "# Call the function\n",
    "df_all_dataflows = get_all_dataflow_details()\n",
    "df_all_dataflows.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DATAFLOWS_ALL\")\n",
    "# display(df_all_dataflows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All DataFlows Gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_dataset_details():\n",
    "    header = pd.DataFrame(columns=[\n",
    "        'DataflowName', 'DataflowID', 'WorkspaceName', 'WorkspaceID', \n",
    "        'ConfiguredBy', 'generation', \n",
    "         'Type', 'Call','isParametric'\n",
    "    ])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    pbiclient = fabric.PowerBIRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    # workspaces_response = pbiclient.get(\"/v1.0/myorg/groups\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Filter workspaces on dedicated capacity\n",
    "    workspaces = [\n",
    "        ws for ws in workspaces \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000'\n",
    "    ]\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            items_response = client.get(f\"/v1/workspaces/{workspaceID}/dataflows\")\n",
    "            #items_response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/dataflows\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Datasets (SemanticModel)\n",
    "            dataflows = items\n",
    "            # dataflows = [item for item in items if item['type'] == 'Dataflow']\n",
    "            \n",
    "            # Process each Dataset\n",
    "            for dataflow in dataflows:\n",
    "                dataflowID = dataflow['id']\n",
    "                dataflowName = dataflow['displayName']\n",
    "                #configuredBy = dataflow['configuredBy']\n",
    "                Type = dataflow['type']\n",
    "                prop = dataflow['properties']\n",
    "                isParametric = prop['isParametric']\n",
    "                try:\n",
    "                    # Error getting datasources - create one row with error\n",
    "                        new_data = {\n",
    "                            'DataflowName': dataflowName, \n",
    "                            'DataflowID': dataflowID, \n",
    "                            'WorkspaceName': workspaceName, \n",
    "                            'WorkspaceID': workspaceID,\n",
    "                            'ConfiguredBy': '',\n",
    "                            'generation': 2,\n",
    "                            'Type': 'Dataflow',\n",
    "                            'Call':'FABRESTAPI',\n",
    "                            'isParametric':isParametric\n",
    "                            }\n",
    "                        df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Dataset {dataflowID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_DS = spark.createDataFrame(df)\n",
    "    return df_DS\n",
    "\n",
    "# Call the function\n",
    "df_all_datasets = get_all_dataset_details()\n",
    "df_all_datasets.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"DATAFLOWS_ALL\")\n",
    "# display(df_all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Workspace users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_all_dataflow_details():\n",
    "    header = pd.DataFrame(columns=[\n",
    "        'userdisplayName', 'emailAddress', 'WorkspaceName', 'WorkspaceID', \n",
    "        'groupUserAccessRight', 'identifier', \n",
    "         'principalType'\n",
    "    ])\n",
    "    df = pd.DataFrame(header)\n",
    "    \n",
    "    client = fabric.FabricRestClient()\n",
    "    pbiclient = fabric.PowerBIRestClient()\n",
    "    \n",
    "    # Get all workspaces\n",
    "    workspaces_response = client.get(\"/v1/workspaces\")\n",
    "    # workspaces_response = pbiclient.get(\"/v1.0/myorg/groups\")\n",
    "    workspaces = workspaces_response.json()['value']\n",
    "    \n",
    "    # Filter workspaces on dedicated capacity\n",
    "    workspaces = [\n",
    "        ws for ws in workspaces \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000'\n",
    "    ]\n",
    "    \n",
    "    # Iterate through all workspaces\n",
    "    for workspace in workspaces:\n",
    "        workspaceID = workspace['id']\n",
    "        workspaceName = workspace['displayName']\n",
    "        \n",
    "        try:\n",
    "            # Get all items in the workspace\n",
    "            # items_response = client.get(f\"/v1/workspaces/{workspaceID}/items\")\n",
    "            items_response = pbiclient.get(f\"/v1.0/myorg/groups/{workspaceID}/users\")\n",
    "            items = items_response.json()['value']\n",
    "            \n",
    "            # Filter for Datasets (SemanticModel)\n",
    "            dataflows = items\n",
    "            # dataflows = [item for item in items if item['type'] == 'Dataflow']\n",
    "            \n",
    "            # Process each Dataset\n",
    "            for dataflow in dataflows:\n",
    "                userdisplayName = dataflow['displayName']\n",
    "                emailAddress = dataflow.get('emailAddress',\"N/A\")\n",
    "                #datasource_id = ds.get('datasourceId', 'N/A')\n",
    "                groupUserAccessRight = dataflow['groupUserAccessRight']\n",
    "                identifier = dataflow['identifier']\n",
    "                principalType=dataflow['principalType']\n",
    "                try:\n",
    "                    # Error getting datasources - create one row with error\n",
    "                        new_data = {\n",
    "                            'userdisplayName': userdisplayName, \n",
    "                            'emailAddress': emailAddress, \n",
    "                            'WorkspaceName': workspaceName, \n",
    "                            'WorkspaceID': workspaceID,\n",
    "                            'groupUserAccessRight': groupUserAccessRight,\n",
    "                            'identifier': identifier,\n",
    "                            'principalType': principalType\n",
    "                            }\n",
    "                        df = pd.concat([df, pd.DataFrame(new_data, index=[0])], ignore_index=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Dataset {dataflowID} in workspace {workspaceName}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing workspace {workspaceName}: {str(e)}\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_DS = spark.createDataFrame(df)\n",
    "    return df_DS\n",
    "\n",
    "# Call the function\n",
    "df_all_dataflows = get_all_dataflow_details()\n",
    "df_all_dataflows.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"WORKSPACE_USERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Users with rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ==========================================\n",
    "# STEP 0: Define Schema\n",
    "# ==========================================\n",
    "schema = StructType([\n",
    "    StructField(\"DatasetName\", StringType(), True),\n",
    "    StructField(\"DatasetID\", StringType(), True),\n",
    "    StructField(\"WorkspaceName\", StringType(), True),\n",
    "    StructField(\"WorkspaceID\", StringType(), True),\n",
    "    StructField(\"configuredBy\", StringType(), True),\n",
    "    StructField(\"identifier\", StringType(), True),\n",
    "    StructField(\"principalType\", StringType(), True),\n",
    "    StructField(\"datasetUserAccessRight\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Clear / Initialize Table\n",
    "# ==========================================\n",
    "table_name = \"DATASET_USERS\"\n",
    "# NOTE: If you are restarting to finish the last 5, COMMENT THIS OUT so you don't lose the first 60!\n",
    "# If starting from scratch, keep it.\n",
    "print(f\" Clearing table '{table_name}' to start fresh...\")\n",
    "spark.createDataFrame([], schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
    "print(f\" Table '{table_name}' is cleared and ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: Fetch Logic (With Timeout)\n",
    "# ==========================================\n",
    "def fetch_users_with_retry(workspaceID, workspaceName, dataset, max_retries=5):\n",
    "    datasetID = dataset['id']\n",
    "    datasetName = dataset['name']\n",
    "    configuredBy = dataset.get('configuredBy', 'N/A')\n",
    "    url = f\"/v1.0/myorg/groups/{workspaceID}/datasets/{datasetID}/users\"\n",
    "    results = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            pbiclient = fabric.PowerBIRestClient()\n",
    "            # 30 second timeout prevents hanging threads\n",
    "            response = pbiclient.get(url, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                users = response.json().get('value', [])\n",
    "                if users:\n",
    "                    for user in users:\n",
    "                        results.append({\n",
    "                            'DatasetName': datasetName, 'DatasetID': datasetID,\n",
    "                            'WorkspaceName': workspaceName, 'WorkspaceID': workspaceID,\n",
    "                            'configuredBy': configuredBy,\n",
    "                            'identifier': user.get('identifier', 'N/A'),\n",
    "                            'principalType': user.get('principalType', 'N/A'),\n",
    "                            'datasetUserAccessRight': user.get('datasetUserAccessRight', 'N/A')\n",
    "                        })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'DatasetName': datasetName, 'DatasetID': datasetID,\n",
    "                        'WorkspaceName': workspaceName, 'WorkspaceID': workspaceID,\n",
    "                        'configuredBy': configuredBy,\n",
    "                        'identifier': 'N/A', 'principalType': 'N/A', 'datasetUserAccessRight': 'N/A'\n",
    "                    })\n",
    "                return results \n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                wait_time = (attempt + 1) * 3 + random.uniform(0, 1)\n",
    "                print(f\" 429 Too Many Requests on '{datasetName}'. Waiting {wait_time:.1f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue \n",
    "\n",
    "            elif response.status_code == 401:\n",
    "                print(f\" Token Expired on '{datasetName}'. Retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue \n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code} on {datasetName}\")\n",
    "                break \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Exception on {datasetName} (Attempt {attempt+1}): {e}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def get_datasets_safe(workspace_id):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            client = fabric.PowerBIRestClient()\n",
    "            resp = client.get(f\"/v1.0/myorg/groups/{workspace_id}/datasets\", timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                return [d for d in resp.json()['value'] if d['name'] != 'SemanticModel']\n",
    "            elif resp.status_code == 401:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            elif resp.status_code == 429:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                return []\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: Main Loop (Aggressive Cleanup)\n",
    "# ==========================================\n",
    "def process_workspaces_incrementally():\n",
    "    client = fabric.FabricRestClient()\n",
    "    ws_resp = client.get(\"/v1/workspaces\")\n",
    "    \n",
    "    # Filter\n",
    "    workspaces = [\n",
    "        ws for ws in ws_resp.json()['value'] \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000' \n",
    "        and not ws['displayName'].startswith('GISC')\n",
    "    ]\n",
    "    \n",
    "    # Sort\n",
    "    workspaces = sorted(workspaces, key=lambda x: x['displayName'])\n",
    "    total_ws = len(workspaces)\n",
    "    \n",
    "    print(f\" Starting processing for {total_ws} workspaces...\")\n",
    "\n",
    "    for i, ws in enumerate(workspaces):\n",
    "        ws_id = ws['id']\n",
    "        ws_name = ws['displayName']\n",
    "        workspace_results = []\n",
    "        \n",
    "        # SKIP LOGIC: If you are restarting and want to skip the first 60, uncomment below:\n",
    "        # if i < 60: continue \n",
    "        \n",
    "        try:\n",
    "            datasets = get_datasets_safe(ws_id)\n",
    "            \n",
    "            if datasets:\n",
    "                # Reduced max_workers to 5 to be gentler on the driver near end of run\n",
    "                with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                    futures = {executor.submit(fetch_users_with_retry, ws_id, ws_name, ds): ds for ds in datasets}\n",
    "                    for future in as_completed(futures):\n",
    "                        data = future.result()\n",
    "                        if data:\n",
    "                            workspace_results.extend(data)\n",
    "                \n",
    "                if workspace_results:\n",
    "                    df_ws = pd.DataFrame(workspace_results)\n",
    "                    spark.createDataFrame(df_ws, schema=schema).write.mode(\"append\").format(\"delta\").saveAsTable(table_name)\n",
    "                    print(f\"   [{i+1}/{total_ws}]  Saved {len(workspace_results)} rows from '{ws_name}'\")\n",
    "            else:\n",
    "                print(f\"   [{i+1}/{total_ws}] No datasets in '{ws_name}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Critical error processing workspace {ws_name}: {e}\")\n",
    "\n",
    "        # --- AGGRESSIVE CLEANUP (Per Workspace) ---\n",
    "        # 1. Force Python to release memory immediately\n",
    "        gc.collect()\n",
    "        # 2. Clear Spark internal cache (optional, but safe)\n",
    "        # spark.catalog.clearCache() \n",
    "        \n",
    "        # --- BATCH PAUSE (Keep this for API limits) ---\n",
    "        if (i + 1) % 10 == 0 and (i + 1) < total_ws:\n",
    "            print(f\"\\n  Processed 10 workspaces. Pausing 3 minutes for API cooldown...\\n\")\n",
    "            time.sleep(60) \n",
    "\n",
    "# Run\n",
    "process_workspaces_incrementally()\n",
    "print(\" Full extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dataset Users and rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ==========================================\n",
    "# STEP 0: Define Schema\n",
    "# ==========================================\n",
    "schema = StructType([\n",
    "    StructField(\"DatasetName\", StringType(), True),\n",
    "    StructField(\"DatasetID\", StringType(), True),\n",
    "    StructField(\"WorkspaceName\", StringType(), True),\n",
    "    StructField(\"WorkspaceID\", StringType(), True),\n",
    "    StructField(\"configuredBy\", StringType(), True),\n",
    "    StructField(\"identifier\", StringType(), True),\n",
    "    StructField(\"principalType\", StringType(), True),\n",
    "    StructField(\"datasetUserAccessRight\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: Clear / Initialize Table\n",
    "# ==========================================\n",
    "table_name = \"DATASET_USERS\"\n",
    "# NOTE: If you are restarting to finish the last 5, COMMENT THIS OUT so you don't lose the first 60!\n",
    "# If starting from scratch, keep it.\n",
    "print(f\" Clearing table '{table_name}' to start fresh...\")\n",
    "spark.createDataFrame([], schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
    "print(f\" Table '{table_name}' is cleared and ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: Fetch Logic (With Timeout)\n",
    "# ==========================================\n",
    "def fetch_users_with_retry(workspaceID, workspaceName, dataset, max_retries=5):\n",
    "    datasetID = dataset['id']\n",
    "    datasetName = dataset['name']\n",
    "    configuredBy = dataset.get('configuredBy', 'N/A')\n",
    "    url = f\"/v1.0/myorg/groups/{workspaceID}/datasets/{datasetID}/users\"\n",
    "    results = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            pbiclient = fabric.PowerBIRestClient()\n",
    "            # 30 second timeout prevents hanging threads\n",
    "            response = pbiclient.get(url, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                users = response.json().get('value', [])\n",
    "                if users:\n",
    "                    for user in users:\n",
    "                        results.append({\n",
    "                            'DatasetName': datasetName, 'DatasetID': datasetID,\n",
    "                            'WorkspaceName': workspaceName, 'WorkspaceID': workspaceID,\n",
    "                            'configuredBy': configuredBy,\n",
    "                            'identifier': user.get('identifier', 'N/A'),\n",
    "                            'principalType': user.get('principalType', 'N/A'),\n",
    "                            'datasetUserAccessRight': user.get('datasetUserAccessRight', 'N/A')\n",
    "                        })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'DatasetName': datasetName, 'DatasetID': datasetID,\n",
    "                        'WorkspaceName': workspaceName, 'WorkspaceID': workspaceID,\n",
    "                        'configuredBy': configuredBy,\n",
    "                        'identifier': 'N/A', 'principalType': 'N/A', 'datasetUserAccessRight': 'N/A'\n",
    "                    })\n",
    "                return results \n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                wait_time = (attempt + 1) * 3 + random.uniform(0, 1)\n",
    "                print(f\" 429 Too Many Requests on '{datasetName}'. Waiting {wait_time:.1f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue \n",
    "\n",
    "            elif response.status_code == 401:\n",
    "                print(f\" Token Expired on '{datasetName}'. Retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue \n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code} on {datasetName}\")\n",
    "                break \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Exception on {datasetName} (Attempt {attempt+1}): {e}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def get_datasets_safe(workspace_id):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            client = fabric.PowerBIRestClient()\n",
    "            resp = client.get(f\"/v1.0/myorg/groups/{workspace_id}/datasets\", timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                return [d for d in resp.json()['value'] if d['name'] != 'SemanticModel']\n",
    "            elif resp.status_code == 401:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            elif resp.status_code == 429:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            else:\n",
    "                return []\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: Main Loop (Aggressive Cleanup)\n",
    "# ==========================================\n",
    "def process_workspaces_incrementally():\n",
    "    client = fabric.FabricRestClient()\n",
    "    ws_resp = client.get(\"/v1/workspaces\")\n",
    "    \n",
    "    # Filter\n",
    "    workspaces = [\n",
    "        ws for ws in ws_resp.json()['value'] \n",
    "        if ws.get('capacityId') and ws['capacityId'] != '00000000-0000-0000-0000-000000000000' \n",
    "        and not ws['displayName'].startswith('GISC')\n",
    "    ]\n",
    "    \n",
    "    # Sort\n",
    "    workspaces = sorted(workspaces, key=lambda x: x['displayName'])\n",
    "    total_ws = len(workspaces)\n",
    "    \n",
    "    print(f\" Starting processing for {total_ws} workspaces...\")\n",
    "\n",
    "    for i, ws in enumerate(workspaces):\n",
    "        ws_id = ws['id']\n",
    "        ws_name = ws['displayName']\n",
    "        workspace_results = []\n",
    "        \n",
    "        # SKIP LOGIC: If you are restarting and want to skip the first 60, uncomment below:\n",
    "        # if i < 60: continue \n",
    "        \n",
    "        try:\n",
    "            datasets = get_datasets_safe(ws_id)\n",
    "            \n",
    "            if datasets:\n",
    "                # Reduced max_workers to 5 to be gentler on the driver near end of run\n",
    "                with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                    futures = {executor.submit(fetch_users_with_retry, ws_id, ws_name, ds): ds for ds in datasets}\n",
    "                    for future in as_completed(futures):\n",
    "                        data = future.result()\n",
    "                        if data:\n",
    "                            workspace_results.extend(data)\n",
    "                \n",
    "                if workspace_results:\n",
    "                    df_ws = pd.DataFrame(workspace_results)\n",
    "                    spark.createDataFrame(df_ws, schema=schema).write.mode(\"append\").format(\"delta\").saveAsTable(table_name)\n",
    "                    print(f\"   [{i+1}/{total_ws}]  Saved {len(workspace_results)} rows from '{ws_name}'\")\n",
    "            else:\n",
    "                print(f\"   [{i+1}/{total_ws}] No datasets in '{ws_name}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Critical error processing workspace {ws_name}: {e}\")\n",
    "\n",
    "        # --- AGGRESSIVE CLEANUP (Per Workspace) ---\n",
    "        # 1. Force Python to release memory immediately\n",
    "        gc.collect()\n",
    "        # 2. Clear Spark internal cache (optional, but safe)\n",
    "        # spark.catalog.clearCache() \n",
    "        \n",
    "        # --- BATCH PAUSE (Keep this for API limits) ---\n",
    "        if (i + 1) % 10 == 0 and (i + 1) < total_ws:\n",
    "            print(f\"\\n  Processed 10 workspaces. Pausing 3 minutes for API cooldown...\\n\")\n",
    "            time.sleep(60) \n",
    "\n",
    "# Run\n",
    "process_workspaces_incrementally()\n",
    "print(\" Full extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset M Query and Columns and Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from notebookutils import mssparkutils\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION & UTILS\n",
    "# ==========================================\n",
    "TMSL_SAVE_PATH = \"/lakehouse/default/Files/PowerBI_TMSLFILES\"\n",
    "\n",
    "# Ensure target directory exists\n",
    "try:\n",
    "    mssparkutils.fs.mkdirs(TMSL_SAVE_PATH)\n",
    "    print(f\" Directory verified/created: {TMSL_SAVE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\" Warning: Could not verify directory {TMSL_SAVE_PATH}. Error: {e}\")\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Removes illegal characters for filenames.\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name).strip()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & SCHEMA DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "# Added \"DOC_EXPRESSIONS\" for items where Enable Load = False\n",
    "TABLES = {\n",
    "    \"measures\": \"DOC_MEASURES\",\n",
    "    \"m_queries\": \"DOC_M_QUERIES\",       # ENABLE LOAD = TRUE\n",
    "    \"expressions\": \"DOC_EXPRESSIONS\",   # ENABLE LOAD = FALSE\n",
    "    \"columns\": \"DOC_COLUMNS\",\n",
    "    \"relationships\": \"DOC_RELATIONSHIPS\",\n",
    "    \"roles\": \"DOC_ROLES\",\n",
    "    \"hierarchies\": \"DOC_HIERARCHIES\"\n",
    "}\n",
    "\n",
    "# Clear tables (Drop to reset schema)\n",
    "print(\" Clearing output tables...\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {t_name}\")\n",
    "for t_name in TABLES.values():\n",
    "    spark.sql(f\"TRUNCATE TABLE {t_name}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. WORKER FUNCTION\n",
    "# ==========================================\n",
    "def parse_dataset_tmsl(row):\n",
    "    ws_id = row['WorkspaceId']\n",
    "    ds_id = row['DatasetId'] \n",
    "    ds_name = row['DatasetName'] \n",
    "    \n",
    "    data = {k: [] for k in TABLES.keys()}\n",
    "    \n",
    "    try:\n",
    "        tmsl_str = fabric.get_tmsl(dataset=ds_id, workspace=ws_id)\n",
    "\n",
    "        # --- NEW: SAVE TO ONELAKE ---\n",
    "        clean_name = sanitize_filename(ds_name)\n",
    "        # Use ds_id to ensure uniqueness in case names duplicate\n",
    "        file_path = f\"{TMSL_SAVE_PATH}/{clean_name}_{ds_id}.json\" \n",
    "        \n",
    "        # Write file using standard Python I/O (Works for OneLake /lakehouse/default paths)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tmsl_str)\n",
    "        # -----------------------------\n",
    "\n",
    "        tmsl = json.loads(tmsl_str)\n",
    "        model = tmsl.get('model', {})\n",
    "        \n",
    "        # --- A. LOADED TABLES (Enable Load = TRUE) ---\n",
    "        if 'tables' in model:\n",
    "            for table in model['tables']:\n",
    "                t_name = table.get('name')\n",
    "                \n",
    "                # 1. M Queries (Partitions)\n",
    "                if 'partitions' in table:\n",
    "                    for p in table['partitions']:\n",
    "                        src = p.get('source', {})\n",
    "                        if src.get('type') == 'm':\n",
    "                            raw_expr = src.get('expression')\n",
    "                            m_code = \"\\n\".join(raw_expr) if isinstance(raw_expr, list) else str(raw_expr)\n",
    "                            data['m_queries'].append({\n",
    "                                \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                                \"TableName\": t_name, \"ObjectName\": p.get('name'), \n",
    "                                \"Type\": \"Table (Loaded)\",  # Explicitly stating this is loaded\n",
    "                                \"EnableLoad\": \"True\",      # <--- KEY FLAG\n",
    "                                \"Expression\": m_code\n",
    "                            })\n",
    "\n",
    "                # 2. Measures\n",
    "                if 'measures' in table:\n",
    "                    for m in table['measures']:\n",
    "                        data['measures'].append({\n",
    "                            \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                            \"TableName\": t_name, \"ObjectName\": m.get('name'), \"Type\": \"Measure\",\n",
    "                            \"Expression\": m.get('expression')\n",
    "                        })\n",
    "\n",
    "                # 3. Columns\n",
    "                if 'columns' in table:\n",
    "                    for c in table['columns']:\n",
    "                        data['columns'].append({\n",
    "                            \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                            \"TableName\": t_name, \"ColumnName\": c.get('name'),\n",
    "                            \"DataType\": c.get('dataType', 'String'), \"Type\": c.get('type', 'Data'),\n",
    "                            \"Expression\": c.get('expression', ''), \"Hidden\": str(c.get('isHidden', False))\n",
    "                        })\n",
    "\n",
    "                # 4. Hierarchies\n",
    "                if 'hierarchies' in table:\n",
    "                    for h in table['hierarchies']:\n",
    "                        levels = [l['name'] for l in h.get('levels', [])]\n",
    "                        data['hierarchies'].append({\n",
    "                            \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                            \"TableName\": t_name, \"HierarchyName\": h.get('name'),\n",
    "                            \"Levels\": \" > \".join(levels)\n",
    "                        })\n",
    "\n",
    "        # --- B. UNLOADED EXPRESSIONS (Enable Load = FALSE) ---\n",
    "        # These are Staging Queries, Parameters, or Functions\n",
    "        if 'expressions' in model:\n",
    "            for expr in model['expressions']:\n",
    "                raw_expr = expr.get('expression')\n",
    "                m_code = \"\\n\".join(raw_expr) if isinstance(raw_expr, list) else str(raw_expr)\n",
    "                \n",
    "                data['expressions'].append({\n",
    "                    \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                    \"ExpressionName\": expr.get('name'),\n",
    "                    \"Kind\": expr.get('kind', 'm'),   # 'm' means Query/Param\n",
    "                    \"EnableLoad\": \"False\",            # <--- KEY FLAG\n",
    "                    \"Expression\": m_code,\n",
    "                    \"Description\": expr.get('description', '')\n",
    "                })\n",
    "\n",
    "        # --- C. MODEL LEVEL (Relationships, Roles) ---\n",
    "        if 'relationships' in model:\n",
    "            for r in model['relationships']:\n",
    "                data['relationships'].append({\n",
    "                    \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                    \"FromTable\": r.get('fromTable'), \"FromColumn\": r.get('fromColumn'),\n",
    "                    \"ToTable\": r.get('toTable'), \"ToColumn\": r.get('toColumn'),\n",
    "                    \"Cardinality\": r.get('cardinality', 'One'), \"IsActive\": str(r.get('isActive', True))\n",
    "                })\n",
    "\n",
    "        if 'roles' in model:\n",
    "            for r in model['roles']:\n",
    "                role_name = r.get('name')\n",
    "                perm = r.get('modelPermission', 'Read')\n",
    "                if 'tablePermissions' in r:\n",
    "                    for tp in r['tablePermissions']:\n",
    "                        data['roles'].append({\n",
    "                            \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                            \"RoleName\": role_name, \"Permission\": perm,\n",
    "                            \"SecuredTable\": tp.get('name'), \"DAXFilter\": tp.get('filterExpression', '')\n",
    "                        })\n",
    "                else:\n",
    "                      data['roles'].append({\n",
    "                        \"WorkspaceId\": ws_id, \"DatasetName\": ds_name, \"DatasetId\": ds_id,\n",
    "                        \"RoleName\": role_name, \"Permission\": perm, \"SecuredTable\": \"All\", \"DAXFilter\": \"\"\n",
    "                    })\n",
    "                    \n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error parsing {ds_name}: {e}\")\n",
    "        return {k: [] for k in TABLES.keys()}\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN ORCHESTRATOR\n",
    "# ==========================================\n",
    "def process_all_datasets_tmsl():\n",
    "    df_input = spark.table(\"LIST_DATASETS\").select(\"WorkspaceId\", \"DatasetId\", \"DatasetName\").distinct().collect()\n",
    "    total_ds = len(df_input)\n",
    "    print(f\" Starting TMSL extraction for {total_ds} datasets...\")\n",
    "    \n",
    "    batch_size = 20\n",
    "    \n",
    "    for i in range(0, total_ds, batch_size):\n",
    "        batch = df_input[i:i + batch_size]\n",
    "        print(f\"   Processing batch {i} to {i + len(batch)}...\")\n",
    "        \n",
    "        batch_results = {k: [] for k in TABLES.keys()}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            futures = [executor.submit(parse_dataset_tmsl, row) for row in batch]\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                for key in TABLES.keys():\n",
    "                    batch_results[key].extend(result[key])\n",
    "\n",
    "        print(f\"    Saving batch data...\")\n",
    "        for key, table_name in TABLES.items():\n",
    "            if batch_results[key]:\n",
    "                df = pd.DataFrame(batch_results[key]).fillna(\"\")\n",
    "                spark.createDataFrame(df).write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n",
    "        \n",
    "        gc.collect()\n",
    "        spark.catalog.clearCache()\n",
    "        time.sleep(5)\n",
    "\n",
    "# Run\n",
    "process_all_datasets_tmsl()\n",
    "print(\" Extraction complete. Check DOC_M_QUERIES (Loaded) and DOC_EXPRESSIONS (Not Loaded).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Storage Details (Lakehose, Warehouse)\n",
    "# Contains both Used and Unused filed and storage in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "from notebookutils import mssparkutils\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TARGET_TABLE_NAME = \"StorageInventory\"\n",
    "\n",
    "# Add the exact names of workspaces you want to skip here\n",
    "EXCLUDED_WORKSPACES = [\n",
    "    \"Dev Sandbox\",\n",
    "    \"HR Sensitive Data\"\n",
    "]\n",
    "\n",
    "# 1. Helper: Physical Size & Count (Recursive Scan)\n",
    "def get_folder_stats(path):\n",
    "    total_bytes = 0\n",
    "    total_files = 0\n",
    "    stack = [path]\n",
    "    loop_count = 0\n",
    "    MAX_LOOPS = 100000 \n",
    "    \n",
    "    try:\n",
    "        while stack:\n",
    "            current_path = stack.pop()\n",
    "            loop_count += 1\n",
    "            if loop_count > MAX_LOOPS: return total_bytes, total_files, \"Limit Reached\"\n",
    "            \n",
    "            try:\n",
    "                items = mssparkutils.fs.ls(current_path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            for item in items:\n",
    "                if item.isDir:\n",
    "                    stack.append(item.path)\n",
    "                else:\n",
    "                    total_bytes += item.size\n",
    "                    total_files += 1\n",
    "                    \n",
    "        return total_bytes, total_files, None\n",
    "    except Exception as e:\n",
    "        return 0, 0, str(e)\n",
    "\n",
    "# 2. Helper: Active Size & Count (Delta Log)\n",
    "def get_active_stats(path):\n",
    "    try:\n",
    "        df = spark.sql(f\"DESCRIBE DETAIL delta.`{path}`\")\n",
    "        row = df.select(\"sizeInBytes\", \"numFiles\").collect()\n",
    "        if row:\n",
    "            return (float(row[0][0]) if row[0][0] else 0.0, \n",
    "                    int(row[0][1]) if row[0][1] else 0)\n",
    "        return 0.0, 0\n",
    "    except AnalysisException:\n",
    "        return 0.0, 0 \n",
    "    except Exception:\n",
    "        return 0.0, 0\n",
    "\n",
    "def scan_tenant_structure():\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    all_results = []\n",
    "\n",
    "    print(\"--- Step 1: Discovering Items via SemPy ---\")\n",
    "    df_workspaces = fabric.list_workspaces()\n",
    "    print(f\"Found {len(df_workspaces)} Workspaces.\")\n",
    "\n",
    "    for index, ws in df_workspaces.iterrows():\n",
    "        ws_name = ws['Name']\n",
    "        ws_id = ws['Id']\n",
    "        \n",
    "        # --- EXCLUSION FILTER ---\n",
    "        if ws_name in EXCLUDED_WORKSPACES:\n",
    "            print(f\"Skipping excluded workspace: {ws_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df_items = fabric.list_items(workspace=ws_id)\n",
    "            df_items = df_items[df_items['Type'].isin(['Lakehouse', 'Warehouse'])]\n",
    "            \n",
    "            if df_items.empty: continue\n",
    "                \n",
    "            print(f\"\\nScanning Workspace: {ws_name}\")\n",
    "\n",
    "            for idx, item in df_items.iterrows():\n",
    "                item_name = item['Display Name']\n",
    "                item_id = item['Id']\n",
    "                item_type = item['Type']\n",
    "                \n",
    "                base_path = f\"abfss://{ws_id}@onelake.dfs.fabric.microsoft.com/{item_id}/Tables\"\n",
    "                \n",
    "                try:\n",
    "                    try:\n",
    "                        root_items = mssparkutils.fs.ls(base_path)\n",
    "                    except:\n",
    "                        continue \n",
    "\n",
    "                    tables_to_process = []\n",
    "                    \n",
    "                    for root_item in root_items:\n",
    "                        if not root_item.isDir: continue\n",
    "                        \n",
    "                        try:\n",
    "                            sub_items = mssparkutils.fs.ls(root_item.path)\n",
    "                            if any(x.name == \"_delta_log\" for x in sub_items):\n",
    "                                tables_to_process.append({\n",
    "                                    \"Schema\": \"dbo\", \"Table\": root_item.name, \"Path\": root_item.path\n",
    "                                })\n",
    "                            else:\n",
    "                                schema_name = root_item.name\n",
    "                                for sub_item in sub_items:\n",
    "                                    if sub_item.isDir:\n",
    "                                        tables_to_process.append({\n",
    "                                            \"Schema\": schema_name, \"Table\": sub_item.name, \"Path\": sub_item.path\n",
    "                                        })\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    for t in tables_to_process:\n",
    "                        full_name = f\"{t['Schema']}.{t['Table']}\"\n",
    "                        print(f\"   > {item_name} -> {full_name}...\", end=\" \")\n",
    "                        \n",
    "                        active_bytes, active_files = get_active_stats(t['Path'])\n",
    "                        active_mb = active_bytes / (1024 * 1024)\n",
    "                        \n",
    "                        phys_bytes, phys_files, error = get_folder_stats(t['Path'])\n",
    "                        phys_mb = phys_bytes / (1024 * 1024)\n",
    "                        \n",
    "                        status = \"Success\"\n",
    "                        if error: \n",
    "                            status = f\"Error: {error}\"\n",
    "                            print(f\"[FAIL]\")\n",
    "                        else:\n",
    "                            print(f\"[Files: {phys_files} | Phys: {phys_mb:.1f} MB]\")\n",
    "                        \n",
    "                        all_results.append({\n",
    "                            \"RunID\": run_id,\n",
    "                            \"ScanTimestamp\": datetime.now(),\n",
    "                            \"WorkspaceName\": ws_name,\n",
    "                            \"ItemName\": item_name,\n",
    "                            \"ItemType\": item_type,\n",
    "                            \"SchemaName\": t['Schema'],\n",
    "                            \"TableName\": t['Table'],\n",
    "                            \"ActiveSizeMB\": float(f\"{active_mb:.2f}\"),\n",
    "                            \"ActiveFileCount\": active_files,\n",
    "                            \"PhysicalSizeMB\": float(f\"{phys_mb:.2f}\"),\n",
    "                            \"PhysicalFileCount\": phys_files,\n",
    "                            \"Status\": status,\n",
    "                            \"FullPath\": t['Path']\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   Error accessing {item_name}: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if all_results:\n",
    "        print(f\"\\nSaving {len(all_results)} records to {TARGET_TABLE_NAME}...\")\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"RunID\", StringType(), True),\n",
    "            StructField(\"ScanTimestamp\", TimestampType(), True),\n",
    "            StructField(\"WorkspaceName\", StringType(), True),\n",
    "            StructField(\"ItemName\", StringType(), True),\n",
    "            StructField(\"ItemType\", StringType(), True),\n",
    "            StructField(\"SchemaName\", StringType(), True),\n",
    "            StructField(\"TableName\", StringType(), True),\n",
    "            StructField(\"ActiveSizeMB\", DoubleType(), True),\n",
    "            StructField(\"ActiveFileCount\", LongType(), True),\n",
    "            StructField(\"PhysicalSizeMB\", DoubleType(), True),\n",
    "            StructField(\"PhysicalFileCount\", LongType(), True),\n",
    "            StructField(\"Status\", StringType(), True),\n",
    "            StructField(\"FullPath\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        df = spark.createDataFrame(all_results, schema)\n",
    "        df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(TARGET_TABLE_NAME)\n",
    "        print(\"Report saved successfully.\")\n",
    "    else:\n",
    "        print(\"No accessible items found.\")\n",
    "scan_tenant_structure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5

}
